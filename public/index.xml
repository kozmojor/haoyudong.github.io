<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home | Haoyu Dong</title>
    <link>http://localhost:1313/</link>
      <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <description>Home</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 24 Oct 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu7729264130191091259.png</url>
      <title>Home</title>
      <link>http://localhost:1313/</link>
    </image>
    
    <item>
      <title>Cross-Modal Cycle-Consistency Rewards for Label-Free Multimodal RL</title>
      <link>http://localhost:1313/post/cross-modal-cycle-consistency-rewards-for-label-free-multimodal-rl/</link>
      <pubDate>Fri, 28 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/cross-modal-cycle-consistency-rewards-for-label-free-multimodal-rl/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/post/cross-modal-cycle-consistency-rewards-for-label-free-multimodal-rl/fig1_modality_gap.png&#34;
    alt=&#34;Modality Gap Example.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Modality Gap Example.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This project is part of my ongoing research in &lt;strong&gt;Multimodal Large Language Model (MLLM) Post-training&lt;/strong&gt;, jointly with &lt;a href=&#34;https://chengzhi-mao.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Prof. Chengzhi Mao&lt;/strong&gt;&lt;/a&gt; (Rutgers) and collaborators.&lt;br&gt;
Our work introduces &lt;strong&gt;Cross-Modal Cycle-Consistency Rewards (R-C¬≤)&lt;/strong&gt; ‚Äî a label-free reinforcement learning framework that converts &lt;strong&gt;image‚Äìtext conflicts&lt;/strong&gt; inside MLLMs into dense, self-generated rewards. This allows a model to improve its visual‚Äìtextual reasoning ability &lt;strong&gt;without human annotations&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/post/cross-modal-cycle-consistency-rewards-for-label-free-multimodal-rl/c3r_cycle_overview.png&#34;
    alt=&#34;Overview of the R-C¬≤ pipeline.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Overview of the R-C¬≤ pipeline.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In summary, R-C¬≤ introduces a simple yet effective self-rewarding paradigm that turns modality contradictions into dense RL signals, enabling consistent improvements across diverse multimodal reasoning tasks.&lt;/p&gt;
&lt;p&gt;R-C¬≤ improves a broad range of benchmarks including &lt;strong&gt;ScienceQA, ChartQA, DocVQA, InfoVQA, MathVista&lt;/strong&gt;, and &lt;strong&gt;A-OKVQA&lt;/strong&gt;, achieving up to &lt;strong&gt;+7.6 accuracy improvement&lt;/strong&gt; under the same model backbone.&lt;br&gt;
The paper is currently &lt;strong&gt;under review at one of the CCF-A conferences&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;üìïüìïüìïThe &lt;strong&gt;preprint version&lt;/strong&gt; of the paper can be found here:&lt;br&gt;
üìöüìöüìö &lt;a href=&#34;https://r-c2-cmlab.github.io/Cross-Modal-Cycle-Consistency-Rewards/static/papers/CVXX2026_Cycle_Consist_arxiv.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Preprint PDF&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;‚≠ê‚≠ê‚≠êFor a more intuitive understanding, you can browse the &lt;strong&gt;interactive project page&lt;/strong&gt; I designed for this work:&lt;br&gt;
üå†üå†üå† &lt;a href=&#34;https://r-c2-cmlab.github.io/Cross-Modal-Cycle-Consistency-Rewards/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Website&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>R-C¬≤: Cross-Modal Cycle Consistency Rewards Improve Multimodal Reasoning</title>
      <link>http://localhost:1313/publication/cross-modal-cycle-consistency-rewards-improve-multimodal-reasoning/</link>
      <pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/cross-modal-cycle-consistency-rewards-improve-multimodal-reasoning/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/publication/cross-modal-cycle-consistency-rewards-improve-multimodal-reasoning/fig1_modality_gap.png&#34;
    alt=&#34;Modality Gap Example.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Modality Gap Example.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This project is part of my ongoing research in &lt;strong&gt;Multimodal Large Language Model (MLLM) Post-training&lt;/strong&gt;, jointly with &lt;a href=&#34;https://chengzhi-mao.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Prof. Chengzhi Mao&lt;/strong&gt;&lt;/a&gt; (Rutgers) and collaborators.&lt;br&gt;
Our work introduces &lt;strong&gt;Cross-Modal Cycle-Consistency Rewards (R-C¬≤)&lt;/strong&gt; ‚Äî a label-free reinforcement learning framework that converts &lt;strong&gt;image‚Äìtext conflicts&lt;/strong&gt; inside MLLMs into dense, self-generated rewards. This allows a model to improve its visual‚Äìtextual reasoning ability &lt;strong&gt;without human annotations&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/publication/cross-modal-cycle-consistency-rewards-improve-multimodal-reasoning/c3r_cycle_overview.png&#34;
    alt=&#34;Overview of the R-C¬≤ pipeline.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Overview of the R-C¬≤ pipeline.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In summary, R-C¬≤ introduces a simple yet effective self-rewarding paradigm that turns modality contradictions into dense RL signals, enabling consistent improvements across diverse multimodal reasoning tasks.&lt;/p&gt;
&lt;p&gt;R-C¬≤ improves a broad range of benchmarks including &lt;strong&gt;ScienceQA, ChartQA, DocVQA, InfoVQA, MathVista&lt;/strong&gt;, and &lt;strong&gt;A-OKVQA&lt;/strong&gt;, achieving up to &lt;strong&gt;+7.6 accuracy improvement&lt;/strong&gt; under the same model backbone.&lt;br&gt;
The paper is currently &lt;strong&gt;under review at one of the CCF-A conferences&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;üìïüìïüìïThe &lt;strong&gt;preprint version&lt;/strong&gt; of the paper can be found here:&lt;br&gt;
üìöüìöüìö &lt;a href=&#34;https://r-c2-cmlab.github.io/Cross-Modal-Cycle-Consistency-Rewards/static/papers/CVXX2026_Cycle_Consist_arxiv.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Preprint PDF&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;‚≠ê‚≠ê‚≠êFor a more intuitive understanding, you can browse the &lt;strong&gt;interactive project page&lt;/strong&gt; I designed for this work:&lt;br&gt;
üå†üå†üå† &lt;a href=&#34;https://r-c2-cmlab.github.io/Cross-Modal-Cycle-Consistency-Rewards/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Website&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Knowleadge Based Multimodal CoT Reasoning</title>
      <link>http://localhost:1313/post/knowleadge-based-multimodal-cot-reasoning/</link>
      <pubDate>Tue, 13 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/knowleadge-based-multimodal-cot-reasoning/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/post/knowleadge-based-multimodal-cot-reasoning/VRLM%20Pipe.png&#34;
    alt=&#34;VRLM Workflow.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;VRLM Workflow.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This project, conducted at Columbia University under &lt;a href=&#34;https://www.aidl.ee.columbia.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Prof. Zoran Kostic&lt;/strong&gt;&lt;/a&gt;, develops a visual reasoning framework to improve the &lt;strong&gt;verifiability and consistency&lt;/strong&gt; of multimodal VQA.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/post/knowleadge-based-multimodal-cot-reasoning/VRLM%20Cap.png&#34;
    alt=&#34;A sample where vanilla caption fails to provides proper information for agent to reason.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;A sample where vanilla caption fails to provides proper information for agent to reason.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The method combines &lt;strong&gt;evidence-dense captioning&lt;/strong&gt;‚Äîwhich fuses multi-view visual cues into a grounded scene description‚Äîwith &lt;strong&gt;agentic self-consistency&lt;/strong&gt; based on rationale-checked majority voting. Evaluated on Commonsense Reasoning benchmark &lt;strong&gt;A-OKVQA&lt;/strong&gt;, the framework yields a &lt;strong&gt;+13 pp&lt;/strong&gt; accuracy gain over strong CoT baselines, with ablations showing substantial contributions from both evidence grounding and agent consensus. The research project can be found [&lt;strong&gt;here&lt;/strong&gt;](Knowleadge Based Multimodal CoT Reasoning.pdf).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>RoPerformer: Rotary Positional Embedding Mechanism on Sparse Attention Architecture</title>
      <link>http://localhost:1313/post/exploring-transformer-models/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/exploring-transformer-models/</guid>
      <description>&lt;p&gt;While transformers handle many tasks remarkably well, their performance heavily depends on how positional information is represented. However, learning long-range and stable positional representations remains difficult‚Äîespecially when we aim to make attention both faster and more scalable.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/post/exploring-transformer-models/Performer.png&#34;
    alt=&#34;Vanilla Attention Mechanism.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Vanilla Attention Mechanism.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This project is advised by &lt;strong&gt;Prof.Krzysztof Choromanski&lt;/strong&gt; as final project for course &lt;strong&gt;IEOR6617 Machine Learning and High-Dimensional Data&lt;/strong&gt; in Columbia, which focuses on the &lt;strong&gt;rotary positional embedding mechanism&lt;/strong&gt; on both classical &lt;strong&gt;Transformer&lt;/strong&gt; and &lt;strong&gt;Sparse-Attention based Transformer(Performer)&lt;/strong&gt;. We conducted thorough experiments involving both models and several &lt;strong&gt;SOTA&lt;/strong&gt; positional embedding mechanisms on &lt;strong&gt;CIFAR100&lt;/strong&gt; dataset and gave detailed analysis in &lt;a href=&#34;exploring-transformer-models.pdf&#34;&gt;&lt;strong&gt;final report&lt;/strong&gt;&lt;/a&gt; on our results.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Maximum Coentropy Criterion based Kalman Filtering</title>
      <link>http://localhost:1313/post/maximum-coentropy-criterion-based-kalman-filtering/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/maximum-coentropy-criterion-based-kalman-filtering/</guid>
      <description>&lt;p&gt;This project is the thesis for my B.S degree in Control &amp;amp; Automation in Xian Jiaotong University, advised by &lt;strong&gt;Prof.Guanghua Zhang&lt;/strong&gt;. The work is mainly about the improvement of &lt;strong&gt;Traditional Kalman Filter(KF)&lt;/strong&gt; under &lt;strong&gt;Non-Gaussian noise&lt;/strong&gt;. In the project, two kinds of non-gaussian noise are introduced as example: Mixture Gaussian Noise &amp;amp; Shot Noise, and Maximum Coentropy Criterion serves as the core of improved KF(MCCKF). Both mathematical derivation &amp;amp; coding experiments show the outstanding performance of MCCKF under given noises compared with classical KF. The Chinese version of the paper can be found &lt;a href=&#34;Maximum-Coentropy-Criterion-based-Kalman-Filtering.pdf&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
