<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications | Haoyu Dong</title>
    <link>http://localhost:1313/publication/</link>
      <atom:link href="http://localhost:1313/publication/index.xml" rel="self" type="application/rss+xml" />
    <description>Publications</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 14 Nov 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu7729264130191091259.png</url>
      <title>Publications</title>
      <link>http://localhost:1313/publication/</link>
    </image>
    
    <item>
      <title>R-CÂ²: Cross-Modal Cycle Consistency Rewards Improve Multimodal Reasoning</title>
      <link>http://localhost:1313/publication/cross-modal-cycle-consistency-rewards-improve-multimodal-reasoning/</link>
      <pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/cross-modal-cycle-consistency-rewards-improve-multimodal-reasoning/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/publication/cross-modal-cycle-consistency-rewards-improve-multimodal-reasoning/fig1_modality_gap.png&#34;
    alt=&#34;Modality Gap Example.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Modality Gap Example.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This project is part of my ongoing research in &lt;strong&gt;Multimodal Large Language Model (MLLM) Post-training&lt;/strong&gt;, jointly with &lt;a href=&#34;https://chengzhi-mao.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Prof. Chengzhi Mao&lt;/strong&gt;&lt;/a&gt; (Rutgers) and collaborators.&lt;br&gt;
Our work introduces &lt;strong&gt;Cross-Modal Cycle-Consistency Rewards (R-CÂ²)&lt;/strong&gt; â€” a label-free reinforcement learning framework that converts &lt;strong&gt;imageâ€“text conflicts&lt;/strong&gt; inside MLLMs into dense, self-generated rewards. This allows a model to improve its visualâ€“textual reasoning ability &lt;strong&gt;without human annotations&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/publication/cross-modal-cycle-consistency-rewards-improve-multimodal-reasoning/c3r_cycle_overview.png&#34;
    alt=&#34;Overview of the R-CÂ² pipeline.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Overview of the R-CÂ² pipeline.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In summary, R-CÂ² introduces a simple yet effective self-rewarding paradigm that turns modality contradictions into dense RL signals, enabling consistent improvements across diverse multimodal reasoning tasks.&lt;/p&gt;
&lt;p&gt;R-CÂ² improves a broad range of benchmarks including &lt;strong&gt;ScienceQA, ChartQA, DocVQA, InfoVQA, MathVista&lt;/strong&gt;, and &lt;strong&gt;A-OKVQA&lt;/strong&gt;, achieving up to &lt;strong&gt;+7.6 accuracy improvement&lt;/strong&gt; under the same model backbone.&lt;br&gt;
The paper is currently &lt;strong&gt;under review at one of the CCF-A conferences&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;ğŸ“•ğŸ“•ğŸ“•The &lt;strong&gt;preprint version&lt;/strong&gt; of the paper can be found here:&lt;br&gt;
ğŸ“šğŸ“šğŸ“š &lt;a href=&#34;https://r-c2-cmlab.github.io/Cross-Modal-Cycle-Consistency-Rewards/static/papers/CVXX2026_Cycle_Consist_arxiv.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Preprint PDF&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;â­â­â­For a more intuitive understanding, you can browse the &lt;strong&gt;interactive project page&lt;/strong&gt; I designed for this work:&lt;br&gt;
ğŸŒ ğŸŒ ğŸŒ  &lt;a href=&#34;https://r-c2-cmlab.github.io/Cross-Modal-Cycle-Consistency-Rewards/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Website&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
