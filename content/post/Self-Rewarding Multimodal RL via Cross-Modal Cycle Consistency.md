---
title: "Cross-Modal Cycle-Consistency Rewards for Label-Free Multimodal RL"
date: 2025-11-28
---

<!--more-->

This project is part of my ongoing research in **multimodal large language model (MLLM) post-training**, jointly with [**Prof. Chengzhi Mao**](https://chengzhi-mao.github.io/) (Rutgers) and collaborators.  
Our work introduces **Cross-Modal Cycle-Consistency Rewards (C³R)** — a *label-free reinforcement learning* framework that converts **image–text conflicts** inside MLLMs into **dense, self-generated rewards**. This allows a model to improve its visual–textual reasoning ability **without human annotations**.

C³R improves a broad range of benchmarks including **ScienceQA, ChartQA, DocVQA, InfoVQA, MathVista**, and **A-OKVQA**, achieving up to **+7.6 accuracy improvement** under the same model backbone.  
The paper is currently **under review at one of the CCF-A conferences**.

The **preprint version** of the paper can be found here:  
[**Preprint PDF**](https://c3r-cmlab.github.io/Cross-Modal-Cycle-Consistency-Rewards/static/papers/CVXX2026_Cycle_Consist_arxiv.pdf)

For a more intuitive understanding, you can browse the **interactive project page** I designed for this work:  
[**Project Website**](https://c3r-cmlab.github.io/Cross-Modal-Cycle-Consistency-Rewards/) 
