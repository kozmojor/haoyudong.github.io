---
title: "Knowleadge Based Multimodal CoT Reasoning"
date: 2025-05-13
---

<!--more-->
This project, conducted at Columbia University under **Prof. Zoran Kostic**, develops a visual reasoning framework to improve the **verifiability and consistency** of multimodal VQA. The method combines **evidence-dense captioning**—which fuses multi-view visual cues into a grounded scene description—with **agentic self-consistency** based on rationale-checked majority voting. Evaluated on A-OKVQA and OK-VQA, the framework yields a **+13 pp** accuracy gain over strong CoT baselines, with ablations showing substantial contributions from both evidence grounding and agent consensus. The research project can be found [**here**](e6691-2025spring-project-VRLM-hd2573-yk3108-wz2708.pdf).